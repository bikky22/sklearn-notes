{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../data/skl.png\">\n",
    "\n",
    "<h1><b>Steps</b><h1>\n",
    "<pre>\n",
    "    |- More than 50 samples\n",
    "    |- Predict a category\n",
    "      |- Labeled Data < 100K samples\n",
    "        |- Linear SVC\n",
    "        |- If Text Data\n",
    "          |- Naive Bayes\n",
    "        |- If not Text Data\n",
    "          |- KNeighbors Classifier\n",
    "          |- SVC > Ensemble Classifiers\n",
    "      |- Labeled Data > 100K samples\n",
    "        |- SGD Classifier\n",
    "        |- Kernel Approximation\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generalized Linear Models</h1>\n",
    "Target is linear combination of inputs\n",
    "\n",
    "Prediction $\\hat{y}$ solves \n",
    "\n",
    "$$\\hat{y}(w,x) = w_{0} + w_{1}x_{1} + \\dots + w_{p}x_{p}$$\n",
    "\n",
    "coef_ $$w = (w_{1}, \\dots , w_{p})$$\n",
    "\n",
    "intercept_ $$w_{0}$$ \n",
    "\n",
    "fit takes array $$ ([X] , [y]) $$\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression  #(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)\n",
    "```\n",
    "> * coef_\n",
    "> * intercept_ \n",
    "\n",
    "| Methods | Parameters | return |\n",
    "|---------|------------|--------|\n",
    "| fit | X, y, sample_weight | self |\n",
    "| get_params | deep | params |\n",
    "| predict | X | C |\n",
    "| score | X, y, sample_weight | score |\n",
    "| set_params | \\*\\*params | self |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares #\n",
    "\n",
    "minimize the residual sum of squares between observed value and value predicted by linear approximation\n",
    "\n",
    "$$\\min_{w} || X_{w} - y || 2^{2}$$\n",
    "\n",
    "> * *Note*: be aware of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Complexity ###\n",
    "Using singular value decomposition of $X$\n",
    "Matrix : $[x]_{n \\times p}$\n",
    "\n",
    "> $\\forall : n \\ge p$\n",
    "\n",
    "> <b>Cost is</b> $O(np^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression #\n",
    "**Ridge Cofficients** : Impose a penalty on the size of coefficients and minimize a penalized residual sum of squares\n",
    "$$\\min_{w} || X_{w} - y || 2^{2} + \\alpha || w || 2^{2}$$\n",
    "\n",
    "> *Complexity parameter* $\\alpha \\ge 0$\n",
    "\n",
    "as **$\\alpha$** increase $\\to$ **Shrinkage** increase $\\to$ coefficients become more robust to collinearity\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge  # parameter = alpha\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha = .5)\n",
    "reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34545455,  0.34545455])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13636363636363641"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Complexity ###\n",
    "Complexity is same as Ordinary Least Squares\n",
    "> <b>Cost is</b> $O(np^{2})$\n",
    "\n",
    "### Setting the regularization parameter: generalized Cross-Validation ###\n",
    "\n",
    "implementation of ** *ridge regression* ** with built-in cross-validation of the alpha parameter\n",
    "\n",
    "\n",
    "works in the same way as <b><i>GridSearchCV</i></b> except that it defaults to Generalized Cross-Validation (GCV), an efficient form of leave-one-out cross-validation\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import RidgeCV  # parameter = list(alphas)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None,\n",
       "    normalize=False, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10000000000000001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso #\n",
    "*a linear model that estimates sparse coefficients (fewer paramater)*\n",
    " Lasso and its variants are fundamental to the field of compressed sensing\n",
    " \n",
    "Uses $\\mathcal{L}_{1}$ regularizer\n",
    " \n",
    "$$\\min_{w} \\frac{1}{2n_{samples}} || X_{w} - y ||_{2} ^{2} + \\alpha || w ||_{1}$$\n",
    "\n",
    "> $\\alpha$ is constant\n",
    "\n",
    "> $||w||_{1}$ is $\\mathcal{L}_{1}$ norm of parametr vector\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso  # parameter = alpha\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha = 0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting regularization parameter ###\n",
    "\n",
    "$\\alpha$ : **alpha** is degree of sparsity of estimated cofficients\n",
    "\n",
    "#### Using cross-validation ####\n",
    "> LassoCV\n",
    ">> High dimentional dataset with many collinear regressors \n",
    "\n",
    "> LassoLarsCV\n",
    ">> based on Least Angle Regression\n",
    "\n",
    ">> Samples is less then features \n",
    "\n",
    "#### Information-criteria based model selection ####\n",
    "LassoLarsIC proposes to use the Akaike information criterion (AIC) and the Bayes Information criterion (BIC)\n",
    "\n",
    "#### Comparison with the regularization parameter of SVM ####\n",
    "$C$ is regularization parameter of SVM\n",
    "\n",
    "$$\\alpha = \\frac{1}{C}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\alpha = \\frac{1}{n_{samples} \\times C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Lasso #\n",
    "estimates sparse coefficients for multiple regression problems jointly \n",
    "\n",
    "<h2>$$[y]_{n_{samples} \\times n_{tasks}}$$</h2>\n",
    "\n",
    "trained with $\\mathcal{L_{1}L_{2}}$ prior as regularizer\n",
    "\n",
    "$$\\min_{w} \\frac{1}{2n_{samples}} || XW - Y || _{Fro} ^{2} + \\alpha ||W|| _{21}$$\n",
    "\n",
    "** * Fro * ** : Frobenius norm\n",
    "$$||A||_{Fro} = \\sqrt{\\sum_{ij} a _{ij} ^{2}}$$\n",
    "\n",
    "** $\\mathcal{L_{1}L_{2}}$ ** : \n",
    "$$||A||_{21} = \\sum_{i}\\sqrt{\\sum_{j}a_{ij}^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net #\n",
    "\n",
    "trained with $\\mathcal{L_{1}}$ and $\\mathcal{L_{2}}$ prior as regularizer\n",
    "\n",
    ">  learning a sparse model where few of the weights are non-zero like <i>Lasso</i>, while still maintaining the regularization properties of <i>Ridge</i>\n",
    "\n",
    "$$\\min_{w} \\frac{1}{2n_{samples}}||X_{w} - y||_{2} ^{2} + \\alpha\\rho || w ||_{1} + \\frac{\\alpha(1 - \\rho)}{2} ||w||_{2}^{2}$$\n",
    "\n",
    "> <i>Note:</i> set the parameters alpha ($\\alpha$) and l1_ratio ($\\rho$) by cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task Elastic Net #\n",
    "estimates sparse coefficients for multiple regression problems jointly \n",
    "\n",
    "<h2>$$[y]_{n_{samples} \\times n_{tasks}}$$</h2>\n",
    "\n",
    "trained with $\\mathcal{L_{1}L_{2}}$ prior and $\\mathcal{L_{2}}$ prior as regularizer\n",
    "\n",
    "$$\\min_{w}\\frac{1}{2n_{samples}} || WX - Y || _{Fro} ^{2} + \\alpha\\rho || W || _{21} + \\frac{\\alpha(1 - \\rho)}{2} || W || _{Fro} ^{2}$$\n",
    "\n",
    "> <i>Note:</i> set the parameters alpha ($\\alpha$) and l1_ratio ($\\rho$) by cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Angle Regression (LARS)#\n",
    "regression algorithm for high-dimensional data\n",
    ">At each step, it finds the predictor most correlated with the response. When there are multiple predictors having equal correlation, instead of continuing along the same predictor, it proceeds in a direction equiangular between the predictors.\n",
    "\n",
    "><i>Note:</i> similar to forward stepwise regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARS Lasso #\n",
    "yields the exact solution which is piecewise linear as a function of the norm of its coefficients.\n",
    "\n",
    "### Mathematical formulation ###\n",
    "Instead of including variables at each step, the estimated parameters are increased in a direction equiangular to each one’s correlations with the residua\n",
    "\n",
    "> <i>Note:</i> similar to forward stepwise regression\n",
    ">> <i>coef\\_path\\_</i> has size of $(n_{features} , max_{features} + 1)$\n",
    "\n",
    ">> first column is zero\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LassoLars  # parameter = alpha\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoLars(alpha=0.1, copy_X=True, eps=2.2204460492503131e-16,\n",
       "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
       "     positive=False, precompute='auto', verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LassoLars(alpha=.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71715729,  0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Matching Pursuit (OMP) #\n",
    "Based on greedy algorithm which approximating the fit of a linear model with constraints imposed on the number of non-zero coefficients (ie. the $\\mathcal{L}_{0}$ pseudo-norm).\n",
    "\n",
    "><i>Note:</i> similar to forward stepwise regression\n",
    "\n",
    "* with fixed non-zero elements\n",
    "\n",
    "$$\\arg\\min||y - X\\gamma||_{2}^{2}$$ \n",
    "$||\\gamma||_{0} \\leq n_{nonzero\\_coefs}$\n",
    "\n",
    "* target specific number of non-zero coefficients\n",
    "\n",
    "$$\\arg\\min ||\\gamma||_{0}$$\n",
    "$|| y - X\\gamma||_{2}^{2} \\leq tol$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression #\n",
    "\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|Xw, \\alpha)$$\n",
    "\n",
    "$\\alpha$ random variable\n",
    "\n",
    "> * adapts to data at hand \n",
    "> * used to include regularization in the estimation procedure (Use <i>uniformative priors</i> over the hyper parameters of the model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression ###\n",
    "also known as classical <i>Ridge</i>\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0, \\lambda^{-1}\\mathcal{I} _{p})$$\n",
    "\n",
    "** *Default :* ** $\\alpha_{1} = \\alpha_{2} = \\lambda_{1} = \\lambda_{2} = 10^{-6}$\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n",
    "Y = [0., 1., 2., 3.]\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50000013])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict ([[1, 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.49999993,  0.49999993])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Relevance Determination - ARD ###\n",
    "> * Sparse Bayesian Learning <br />\n",
    "> * Relevance Vector Machine\n",
    "\n",
    "$$p(w|\\lambda) = \\mathcal{N}(w|0, A^{-1})$$\n",
    "$diag(A) = \\lambda = \\{ \\lambda_{1}, \\dots \\lambda_{p}\\}$\n",
    "distribution over $w$ is assumed to be axis-parallel elliptical Gaussian distribution.\n",
    "* *center is (0,0)*\n",
    "* *precision is $\\lambda_{\\iota}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression #\n",
    "\n",
    "AKA:\n",
    "* logit regression\n",
    "* maximum-entropy classification (MaxEnt)\n",
    "* log-linear classifier\n",
    "\n",
    "** *$\\mathcal{L}_{2}$ penalized logistic regression* **\n",
    "$$\\min_{w,c}\\frac{1}{2}w^{T}w + C \\sum _{i = 1} ^{n}\\log(\\exp(-y_{i}(X_{i}^{T}w+c))+1)$$\n",
    "\n",
    "** *$\\mathcal{L}_{1}$ regularized logistic regression* **\n",
    "$$\\min_{w,c}||w||_{1}+C\\sum_{i=1}^{n}\\log(\\exp({-y_{i}}(X_{i}^{T}w+c))+1)$$\n",
    "\n",
    "| Case | Solver |\n",
    "|------|------|\n",
    "| L1 penalty | liblinear, saga |\n",
    "| Multinomial loss | lbfgs, sag, saga, newton-cg |\n",
    "| Very Large dataset (n_samples) | sag, saga |\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```\n",
    "\n",
    "> * coef_\n",
    "> * intercept_\n",
    "> * n_iter_\n",
    "\n",
    "| Parameters | type | value | dafault |\n",
    "|------------|------|-------|---------|\n",
    "| penalty | str | ‘l1’ or ‘l2’ | ‘l2’ |\n",
    "| dual | bool |True or False | False |\n",
    "| tol | float  ||1e-4|\n",
    "| C | float | | 1.0 |\n",
    "| fit_intercept | bool | | True |\n",
    "| intercept_scaling | float | | 1 |\n",
    "| class_weight | dict | ‘balanced’ | None |\n",
    "| random_state | int | RandomState instance or None, optional | None |\n",
    "| solver | str | {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} | ‘liblinear’ |\n",
    "| max_iter | int | | 100 |\n",
    "| multi_class | str | {‘ovr’, ‘multinomial’} | ‘ovr’ |\n",
    "| verbose | int| | 0|\n",
    "| warm_start | bool| | False |\n",
    "| n_jobs | int| | 1 |\n",
    "\n",
    "| Methods | Parameters | return |\n",
    "|---------|------------|--------|\n",
    "| decision_function | X | array |\n",
    "| densify | | self |\n",
    "| fit | X, y, sample_weight=None | self |\n",
    "| get_params | deep = true | params |\n",
    "| predict | X | c |\n",
    "| predict_log_proba | X | T |\n",
    "| predict_proba | X | T |\n",
    "| score | X, y, sample_weight=None | score |\n",
    "| set_params | \\*\\*params | self | \n",
    "| sparsify | | self |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent - SGD\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "```\n",
    "\n",
    "use it when number of samples or features is very large"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
